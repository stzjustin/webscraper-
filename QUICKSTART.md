# âš¡ QUICKSTART - Web Scraper Pro

**5-Minuten Setup fÃ¼r AnfÃ¤nger - Dein Mac mit Silicon Chip**

---

## ğŸ¯ Was du bekommst

âœ… Crawle komplette Websites automatisch
âœ… Erstelle PDFs mit intelligenten Namen (inkl. Keywords)
âœ… Perfekt fÃ¼r n8n â†’ LLM â†’ Google Drive â†’ Pinecone Workflows
âœ… AusfÃ¼hrbare Anwendung (lÃ¤uft ohne Python-Installation)

---

## ğŸš€ Schnellster Start (5 Minuten)

### Schritt 1: Terminal Ã¶ffnen
1. DrÃ¼cke `Cmd + Space`
2. Tippe: `terminal`
3. DrÃ¼cke `Enter`

### Schritt 2: In Projekt-Ordner wechseln
```bash
cd ~/Downloads/webscraper-  # Oder wo auch immer du es gespeichert hast
```

### Schritt 3: Quickstart-Skript ausfÃ¼hren
```bash
./quickstart_mac.sh
```

**Das war's! ğŸ‰**

Das Skript:
- âœ… PrÃ¼ft Python
- âœ… Erstellt Virtual Environment
- âœ… Installiert alle Dependencies
- âœ… Startet den Scraper

---

## ğŸ“ Verwendung

Nach dem Start fragt das Programm:

```
Website URL (e.g., https://example.com):
```
â¡ï¸ Gib deine Website ein, z.B. `https://ihre-website.de`

```
Max URLs to crawl (e.g., 20, 50, 100):
```
â¡ï¸ Gib die Anzahl ein, z.B. `20`

```
20 URLs found and saved.
Create PDFs now? (yes/no):
```
â¡ï¸ Tippe `yes` und drÃ¼cke Enter

âœ… **Fertig!** PDFs werden erstellt und auf deinem Desktop gespeichert:
`~/Desktop/WebScraperPDFs/`

---

## ğŸ“¦ Als Standalone-App erstellen

### FÃ¼r deinen Mac (Silicon)

```bash
./build_mac.sh
```

**Ergebnis:** `dist/WebScraperPro`

**Starten:**
```bash
./dist/WebScraperPro
```

**Optional - Als App installieren:**
```bash
sudo cp -r dist/WebScraperPro.app /Applications/
```

Dann findest du es in deinen Anwendungen! ğŸ‰

---

## ğŸ” Beim nÃ¤chsten Mal starten

### Option 1: Mit Virtual Environment
```bash
cd ~/Downloads/webscraper-
source venv/bin/activate
python web_scraper_pro.py
```

### Option 2: Quickstart-Skript (einfacher!)
```bash
cd ~/Downloads/webscraper-
./quickstart_mac.sh
```

### Option 3: Standalone-App (nach Build)
```bash
./dist/WebScraperPro
```

Oder doppelklick auf `WebScraperPro.app` im Finder!

---

## ğŸ’¡ Beispiel-Workflow

### FÃ¼r eine Yoga-Studio-Website

```bash
Website URL: https://yoga-studio-beispiel.de
Max URLs: 30

# Scraper findet automatisch:
# - Homepage
# - Ãœber uns
# - Kurse
# - Preise
# - Kontakt
# - Blog-Artikel
# - usw.

# Erstellt PDFs wie:
# 001_20250107_143052_yoga_kurse_studio_yoga-studio-beispiel_de.pdf
# 002_20250107_143055_preise_mitgliedschaft_yoga-studio-beispiel_de.pdf
# 003_20250107_143058_kontakt_anfahrt_yoga-studio-beispiel_de.pdf
```

### Diese PDFs dann:
1. â¡ï¸ n8n Workflow
2. â¡ï¸ LLM (OpenAI/Anthropic) fÃ¼r Zusammenfassung
3. â¡ï¸ Umbenennen mit LLM-Output
4. â¡ï¸ Google Drive Upload
5. â¡ï¸ Pinecone Embedding fÃ¼r Chatbot

---

## â“ HÃ¤ufige Probleme

### "Python not found"
**LÃ¶sung:**
```bash
brew install python3
```

### "Chrome not found"
**LÃ¶sung:** Installiere Chrome:
[google.com/chrome](https://www.google.com/chrome/)

### "Permission denied"
**LÃ¶sung:**
```bash
chmod +x quickstart_mac.sh
./quickstart_mac.sh
```

### "ChromeDriver Fehler"
**LÃ¶sung:**
```bash
brew install chromedriver
xattr -d com.apple.quarantine /opt/homebrew/bin/chromedriver
```

---

## ğŸ“Š Output

### Desktop-Ordner
```
~/Desktop/WebScraperPDFs/
â”œâ”€â”€ 001_..._keywords_domain.pdf  â† Seitennummer, Timestamp, Keywords, Domain
â”œâ”€â”€ 002_..._keywords_domain.pdf
â”œâ”€â”€ ...
â”œâ”€â”€ scraped_urls.json            â† Alle gefundenen URLs
â””â”€â”€ scraper_....log              â† Log-Datei
```

### PDF-Inhalt
Jedes PDF enthÃ¤lt:
- ğŸ“ URL
- ğŸ• Timestamp
- ğŸ”‘ 3 Keywords (im Dateinamen UND im PDF)
- ğŸ“„ Seitenzahl (z.B. "Seite 5 von 30")
- ğŸ“ Bereinigten Content

---

## ğŸ¯ Best Practice fÃ¼r n8n

### 1. Kleine Mengen testen
```
Max URLs: 10  # Statt 100
```

### 2. Zeitplan einrichten
Crawle nachts oder zu ruhigen Zeiten

### 3. Delay erhÃ¶hen fÃ¼r groÃŸe Sites
```yaml
# In config.yaml
delay_between_requests: 3.0  # Statt 2.0
```

### 4. URLs sichern
```bash
cp ~/Desktop/WebScraperPDFs/scraped_urls.json ~/Backups/
```

---

## ğŸ“š Weitere Infos

**AusfÃ¼hrliche Anleitung:**
â¡ï¸ Siehe `README_PRO.md`

**n8n Integration:**
â¡ï¸ Siehe Kapitel "n8n Integration" in `README_PRO.md`

**Troubleshooting:**
â¡ï¸ Siehe Kapitel "Troubleshooting" in `README_PRO.md`

**Konfiguration:**
â¡ï¸ Kopiere `config_example.yaml` zu `config.yaml` und editiere

---

## ğŸ†˜ Support

**Problem gefunden?**
Erstelle ein Issue auf GitHub mit:
- Python-Version (`python3 --version`)
- macOS-Version (`sw_vers`)
- Log-Datei (aus `~/Desktop/WebScraperPDFs/`)

---

## ğŸ‰ Du bist startklar!

```bash
./quickstart_mac.sh
```

**Viel Erfolg mit deinem Web Scraper Pro!**

---

## ğŸ“‹ Cheat Sheet

```bash
# Setup (nur einmal)
./quickstart_mac.sh

# Scraper starten
python web_scraper_pro.py

# Executable bauen
./build_mac.sh

# Virtual Environment aktivieren
source venv/bin/activate

# Virtual Environment deaktivieren
deactivate

# Dependencies neu installieren
pip install -r requirements-pro.txt

# ChromeDriver installieren
brew install chromedriver
xattr -d com.apple.quarantine /opt/homebrew/bin/chromedriver
```

---

**ğŸš€ Viel Erfolg!**
